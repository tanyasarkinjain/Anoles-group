{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modified version of Cleaned_Up_Blurred_Otsu, with functions rearranged.'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Modified version of Cleaned_Up_Blurred_Otsu, with functions rearranged.'''\n",
    "\n",
    "# problems:\n",
    "# Thresholding on inverted images seems to produce low quality results\n",
    "# not enough info from some of the images if not enough non-edge scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from os import scandir\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi'] = 120\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "import pandas\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_otsu(img):\n",
    "    '''Parameter img should already have been read by cv2 and converted to grayscale.\n",
    "    Returns sure foreground of image thresholded using Otsu.'''\n",
    "    \n",
    "    # Apply blur\n",
    "    blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "    \n",
    "    # Threshold image using Otsu\n",
    "    ret, thresh = cv2.threshold(blur,0 , 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Perform opening on thresholded image\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 4)\n",
    "\n",
    "    # Finding sure foreground area\n",
    "    dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "    ret, sure_fg = cv2.threshold(dist_transform, 1.5, 255, 0)\n",
    "    \n",
    "    return sure_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_blocksize_and_iterations(img):\n",
    "    '''Parameter img should be the sure foreground of the Otsu-thresholded image.\n",
    "    Returns selected blocksize and iterations.'''\n",
    "    \n",
    "    # Collect stats from image\n",
    "    binary_map = (img > 0).astype(np.uint8)\n",
    "    connectivity = 4 # can be changed\n",
    "    output = cv2.connectedComponentsWithStats(binary_map, connectivity, cv2.CV_32S)\n",
    "   \n",
    "    num_labels = output[0]\n",
    "    labels = output[1]\n",
    "    stats = output[2]\n",
    "    centroids = output[3]\n",
    "    \n",
    "    avg_scale_size = np.average(stats[1:,-1])\n",
    "    \n",
    "    if num_labels <= 3:\n",
    "        return 15, 1\n",
    "    elif avg_scale_size < 350:\n",
    "        return 65, 2\n",
    "    elif avg_scale_size < 550:\n",
    "        return 65, 3\n",
    "    else:\n",
    "        return 85, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_custom(img, blocksize, iterations):\n",
    "    '''Parameter img should already have been read by cv2 and converted to grayscale.'''\n",
    "\n",
    "    if (blocksize > 15):\n",
    "        blur = cv2.GaussianBlur(img,(5,5),1)\n",
    "        blur = cv2.bilateralFilter(blur,10,10,100)\n",
    "    else:\n",
    "        blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "\n",
    "    thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blocksize, -2)\n",
    "\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations=iterations)\n",
    "\n",
    "    # Finding sure foreground area\n",
    "    dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "    ret, sure_fg = cv2.threshold(dist_transform, 1.5, 255, 0) \n",
    "\n",
    "    # Collect stats from sure_foreground image\n",
    "    binary_map = (sure_fg > 0).astype(np.uint8)\n",
    "    connectivity = 4 # can be changed\n",
    "    output = cv2.connectedComponentsWithStats(binary_map, connectivity, cv2.CV_32S)\n",
    "   \n",
    "    num_labels = output[0]\n",
    "    labels = output[1]\n",
    "    stats = output[2]\n",
    "    centroids = output[3]\n",
    "    \n",
    "    scale_sizes = np.copy(stats[1:,-1]) # start from 1 instead of 0 to ignore background, use copy so that stats array doesn't get modified\n",
    "    scale_sizes.sort() # sort the list of scale sizes from smallest to largest\n",
    "    avg_scale_size = np.average(scale_sizes[len(scale_sizes) // 2:]) # average size among the largest half of the scales\n",
    "    \n",
    "    # Identifying noise\n",
    "    noise_labels = []\n",
    "    for i in range(num_labels):\n",
    "        # if the scale is smaller than 1/10 the average among the largest half of the scales, save it for removal\n",
    "        if stats[i, -1] < (avg_scale_size / 10):\n",
    "            noise_labels.append(i)\n",
    "            num_labels -= 1\n",
    "    # Remove the identified noise\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] in noise_labels:\n",
    "                labels[i][j] = 0 # turn noise pixels into background pixels\n",
    "    # Note: Removed noise from labels matrix, but the noise is still in stats/centroids matrices\n",
    "    \n",
    "    # Label and count\n",
    "    # source: https://medium.com/analytics-vidhya/images-processing-segmentation-and-objects-counting-in-an-image-with-python-and-opencv-216cd38aca8e\n",
    "    label_hue = np.uint8(179 * labels / np.max(labels))\n",
    "    blank_ch = 255 * np.ones_like(label_hue)\n",
    "    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
    "    labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
    "    labeled_img[label_hue == 0] = 0\n",
    "    \n",
    "    return sure_fg, labeled_img, num_labels - 1 # background doesn't count as a scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results_orig(orig, with_noise, noise_removed, count):\n",
    "    plt.subplot(1,3,1), plt.imshow(orig, 'gray')\n",
    "    plt.title(\"Orginal Grayscale\", fontsize=5)\n",
    "    plt.subplot(1,3,2), plt.imshow(with_noise, 'gray')\n",
    "    plt.title(\"With Noise\", fontsize=5)\n",
    "    plt.subplot(1,3,3), plt.imshow(noise_removed, 'gray')\n",
    "    plt.title(\"Noise-Removed, Count = \" + str(count), fontsize=5)\n",
    "    plt.show()\n",
    "\n",
    "def display_results_invert(orig, invert, with_noise, noise_removed, count):\n",
    "    plt.subplot(1,4,1), plt.imshow(orig, 'gray')\n",
    "    plt.title(\"Orginal Grayscale\", fontsize=5)\n",
    "    plt.subplot(1,4,2), plt.imshow(invert, 'gray')\n",
    "    plt.title(\"Inverted\", fontsize=5)\n",
    "    plt.subplot(1,4,3), plt.imshow(with_noise, 'gray')\n",
    "    plt.title(\"Inverted With Noise\", fontsize=5)\n",
    "    plt.subplot(1,4,4), plt.imshow(noise_removed, 'gray')\n",
    "    plt.title(\"Inverted Noise-Removed, Count = \" + str(count), fontsize=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    binary_map = (img > 0).astype(np.uint8) # make the labeled image binary (make everything that is not black be white)\n",
    "    \n",
    "    output = cv2.connectedComponentsWithStats(binary_map, 4, cv2.CV_32S)\n",
    "    num_labels = output[0]\n",
    "    labels = output[1]\n",
    "    stats = output[2]\n",
    "    centroids = output[3]\n",
    "    size_var = scale_size_variation(num_labels, stats, len(img), len(img[0]))\n",
    "    overlap = overlap_of_bounds(num_labels, stats, len(img), len(img[0])) #bounding box overlap not that useful - maybe only use to eliminate if extremely high, like > 1?\n",
    "    fullness, squareness = fullness_and_squareness(num_labels, stats, len(img), len(img[0]))\n",
    "    \n",
    "    return size_var, overlap, fullness, squareness\n",
    "\n",
    "    \n",
    "def bounding_box_edges(i, stats):\n",
    "     # find edges of bounding box\n",
    "        left = stats[i, cv2.CC_STAT_LEFT]\n",
    "        right = left + stats[i, cv2.CC_STAT_WIDTH] - 1\n",
    "        top = stats[i, cv2.CC_STAT_TOP]\n",
    "        bottom = top + stats[i, cv2.CC_STAT_HEIGHT] - 1\n",
    "        return left, right, top, bottom\n",
    "        \n",
    "def scale_size_variation(num_labels, stats, img_len, img_width):\n",
    "    scale_areas = []\n",
    "    for i in range(1, num_labels): # start at 1 to ignore background\n",
    "        \n",
    "        left, right, top, bottom = bounding_box_edges(i, stats)\n",
    "        # don't consider scales that touch the edge\n",
    "        if left == 0 or top == 0 or right == img_width - 1 or bottom == img_len - 1:\n",
    "            continue\n",
    "            \n",
    "        scale_areas.append(stats[i, cv2.CC_STAT_AREA].astype(int))\n",
    "\n",
    "    if len(scale_areas) > 1:\n",
    "        stddev = statistics.stdev(scale_areas)\n",
    "        mean = sum(scale_areas) / len(scale_areas)\n",
    "        #print(\"(Std dev / mean) for scale area: \" + str(stddev / mean))\n",
    "       \n",
    "        # thresholding good if return value less than 1\n",
    "        return (stddev / mean)\n",
    "        # returns None if less than 2 non-edge scales\n",
    "        \n",
    "def overlap_of_bounds(num_labels, stats, img_len, img_width):\n",
    "    if num_labels < 2: # return None if no scales\n",
    "        return\n",
    "    #check for overlappingness of bounding boxes\n",
    "    overlap_array = np.zeros((img_len, img_width))\n",
    "    for i in range(1, num_labels): # start at 1 to ignore background\n",
    "        left, right, top, bottom = bounding_box_edges(i, stats)\n",
    "        for r in range(top, bottom + 1):\n",
    "            for c in range(left, right + 1):\n",
    "                overlap_array[r, c] += 1\n",
    "    avg_overlap = np.average(overlap_array)\n",
    "    #print(\"Avg bounding box overlap: \" + str(avg_overlap))\n",
    "    percent_overlap = np.count_nonzero(overlap_array > 1) / np.size(overlap_array)\n",
    "    \n",
    "    # thresholding good if return value less than 0.63\n",
    "    return avg_overlap\n",
    "    \n",
    "def fullness_and_squareness(num_labels, stats, img_len, img_width):\n",
    "    \n",
    "    squareness = []\n",
    "    fullness = []\n",
    "    for i in range(1, num_labels):\n",
    "        \n",
    "        left, right, top, bottom = bounding_box_edges(i, stats)\n",
    "        # don't consider scales that touch the edge\n",
    "        if left == 0 or top == 0 or right == img_width - 1 or bottom == img_len - 1:\n",
    "            continue\n",
    "            \n",
    "        w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "        h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "        squareness.append(min(w, h) / max(w, h)) # closer to 1 means more square, smaller than 1 means less square\n",
    "        \n",
    "        scale_area = stats[i, cv2.CC_STAT_AREA]\n",
    "        box_area = w * h\n",
    "        fullness.append(scale_area / box_area)\n",
    "    if fullness:\n",
    "        avg_fullness = sum(fullness) / len(fullness)\n",
    "        #print(\"Avg fullness: \" + str(avg_fullness))\n",
    "        squareness.sort()\n",
    "        least_square = squareness[:10]\n",
    "        avg_squareness = sum(least_square) / len(least_square)\n",
    "        #print(\"Avg squareness of 10 least square scales: \" + str(avg_squareness))\n",
    "    \n",
    "        # thresholding good if return value > 0.7\n",
    "        return avg_fullness, avg_squareness\n",
    "    else:\n",
    "        return None, None\n",
    "    # returns None if no non-edge scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(result_list):\n",
    "    '''Compares the data from different thresholding attempts and returns the index corresponding to the best.\n",
    "    Each element in parameter result_list is a list of the form [size_var, overlap, fullness, squareness]\n",
    "    resulting from analysis of a thresholding attempt.'''\n",
    "    orig_data = result_list[0]\n",
    "    inverted_data = result_list[1]\n",
    "    \n",
    "    size_var = [orig_data[0], inverted_data[0]]\n",
    "    overlap = [orig_data[1], inverted_data[1]]\n",
    "    fullness = [orig_data[2], inverted_data[2]]\n",
    "    squareness = [orig_data[3], inverted_data[3]]\n",
    "    \n",
    "    orig, inverted = 0, 0\n",
    "    \n",
    "    #adjust the weights? - currently larger differences count more, but all 4 criteria (size_var, overlap, etc.) are weighted equally\n",
    "    # Maybe size variation should have greatest weight?\n",
    "    # Compare size variation\n",
    "    if size_var[0] and size_var[1]:\n",
    "        if size_var[0] < size_var[1]:\n",
    "            orig += size_var[1] - size_var[0]\n",
    "        elif size_var[0] > size_var[1]:\n",
    "            inverted += size_var[0] - size_var[1]\n",
    "    else: # if one or more the results don't have data on size variation (b/c less than 2 non-edge scales)\n",
    "        return 0 # scale size variation is very important, so we can't make conclusions - when in doubt, keep original image\n",
    "    \n",
    "    # Compare overlap of bounding boxes\n",
    "    if overlap[0] and overlap[1]:\n",
    "        if overlap[0] < overlap[1]:\n",
    "            orig += overlap[1] - overlap[0]\n",
    "        if overlap[0] > overlap[1]:\n",
    "            inverted += overlap[0] - overlap[1]\n",
    "        \n",
    "    # Compare fullness (scale to bounding box ratio)\n",
    "    if fullness[0] and fullness[1]:\n",
    "        if fullness[0] > fullness[1]:\n",
    "            orig += fullness[0] - fullness[1]\n",
    "        elif fullness[0] < fullness[1]:\n",
    "            inverted += fullness[1] - fullness[0]\n",
    "    \n",
    "    # Compare squareness of bounding boxes\n",
    "    if squareness[0] and squareness[1]:\n",
    "        if squareness[0] > squareness[1]:\n",
    "            orig += squareness[0] - squareness[1]\n",
    "        elif squareness[0] < squareness[1]:\n",
    "            inverted += fullness[1] - fullness[0]\n",
    "    \n",
    "    # Return the index corresponding to the better result\n",
    "    if orig > inverted:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(img):\n",
    "    return cv2.bitwise_not(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_scales(img):\n",
    "    '''Parameter img must already have been read into cv2 and converted to grayscale.\n",
    "    Calls threshold_otsu, then choose_blocksize_and_iterations, then threshold_custom.\n",
    "    Returns resulting images and count.'''\n",
    "    img_otsu = threshold_otsu(img)\n",
    "    blocksize, iterations = choose_blocksize_and_iterations(img_otsu)\n",
    "    print(\"Blocksize, iterations: \" + str(blocksize) + \", \" + str(iterations))\n",
    "    with_noise, labeled_img, count = threshold_custom(img, blocksize, iterations)\n",
    "    return with_noise, labeled_img, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0bf11419b2c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Guanica_County_images/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.ipynb_checkpoints'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.DS_Store'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# need to figure how to remove these files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "dirname = 'Guanica_County_images/'\n",
    "directory = os.scandir(dirname)\n",
    "for img in directory:\n",
    "    if (img.name == '.ipynb_checkpoints' or img.name == '.DS_Store'): # need to figure how to remove these files\n",
    "        continue\n",
    "    img = cv2.imread(dirname + img.name)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # count scales on original image\n",
    "    with_noise, labeled_img, count = count_scales(img)\n",
    "    \n",
    "    # check how good the results are\n",
    "    size_var, overlap, fullness, squareness = analyze_results(labeled_img)\n",
    "    \n",
    "    # If the result is good, display results and finish\n",
    "    if (size_var and overlap and fullness and squareness and\n",
    "        size_var < 1 and overlap < 0.63 and fullness > 0.7 and squareness > 0.5):\n",
    "            print(\"Finished on first try.\")\n",
    "            display_results_orig(img, with_noise, labeled_img, count)\n",
    "            \n",
    "    # Otherwise, try inverted image and compare results to original\n",
    "    else:\n",
    "        # save the original results in lists for later use\n",
    "        result_images = [[img, with_noise, labeled_img, count]]\n",
    "        result_data = [[size_var, overlap, fullness, squareness]]\n",
    "    \n",
    "        # invert image\n",
    "        inverted_img = invert(img)\n",
    "    \n",
    "        # count scales on inverted image\n",
    "        with_noise, labeled_img, count = count_scales(inverted_img)\n",
    "    \n",
    "        # check how good the new results are\n",
    "        size_var, overlap, fullness, squareness = analyze_results(labeled_img)\n",
    "        result_data.append([size_var, overlap, fullness, squareness])\n",
    "        \n",
    "        # compare the results from original and inverted images\n",
    "        best_index = compare_results(result_data)\n",
    "    \n",
    "        if best_index == 0:\n",
    "            print(\"Tried both original and inverted, but original was better.\")\n",
    "            display_results_orig(result_images[0][0], result_images[0][1], result_images[0][2], result_images[0][3])\n",
    "        else:\n",
    "            print(\"Decided to used inverted image.\")\n",
    "            display_results_invert(img, inverted_img, with_noise, labeled_img, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
